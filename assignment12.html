

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Classification of Fake News &#8212; Intro to Data Science Portfolio</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'assignment12';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="&lt;no title&gt;" href="a10_comparison.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
  
    <p class="title logo__title">Intro to Data Science Portfolio</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    Introduction to Data Science Portfolio
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Assignments</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="a5_us_presidential_elections.html">Web-Scraping Presidential Information</a></li>

<li class="toctree-l1"><a class="reference internal" href="a6_adults.html">Accuracy for GPR income&gt;=20k</a></li>







<li class="toctree-l1"><a class="reference internal" href="Assignment7.html">7.2 – Dataset and EDA</a></li>


<li class="toctree-l1"><a class="reference internal" href="assignment8.html">Evaluating Our Clusters</a></li>


<li class="toctree-l1"><a class="reference internal" href="assignment9.html">Dataset Imports and Cleaning</a></li>










<li class="toctree-l1 current active"><a class="current reference internal" href="#">Classification of Fake News</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/assignment12.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Classification of Fake News</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#titles-vector-representation-score">Titles Vector Representation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#texts-vector-representation-score">Texts Vector Representation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#titles-tf-idf-representation-score">Titles TF-IDF Representation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#texts-tf-idf-representation-score">Texts TF-IDF Representation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#titles-vector-cross-validation-score">Titles Vector Cross-Validation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#texts-vector-cross-validation-score">Texts Vector Cross-Validation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#titles-tf-idf-cross-validation-score">Titles TF-IDF Cross-Validation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#texts-tf-idf-cross-validation-score">Texts TF-IDF Cross-Validation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-optimization">Model Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-visualization">Data Visualization</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="classification-of-fake-news">
<h1>Classification of Fake News<a class="headerlink" href="#classification-of-fake-news" title="Permalink to this heading">#</a></h1>
<p>First we import all of our metrics and functions that we will need for this analysis. This includes our standard libraries, our euclidean vector analysis tool, and our classification methods.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction</span><span class="w"> </span><span class="kn">import</span> <span class="n">text</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics.pairwise</span><span class="w"> </span><span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">MultinomialNB</span><span class="p">,</span> <span class="n">GaussianNB</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span>
</pre></div>
</div>
<p>We then import our dataset in its original form from the given csv file. This will be stored as news_df.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">news_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;fake_or_real_news.csv&#39;</span><span class="p">)</span>
<span class="n">news_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>title</th>
      <th>text</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8476</td>
      <td>You Can Smell Hillary’s Fear</td>
      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>
      <td>FAKE</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10294</td>
      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>
      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>
      <td>FAKE</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3608</td>
      <td>Kerry to go to Paris in gesture of sympathy</td>
      <td>U.S. Secretary of State John F. Kerry said Mon...</td>
      <td>REAL</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10142</td>
      <td>Bernie supporters on Twitter erupt in anger ag...</td>
      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>
      <td>FAKE</td>
    </tr>
    <tr>
      <th>4</th>
      <td>875</td>
      <td>The Battle of New York: Why This Primary Matters</td>
      <td>It's primary day in New York and front-runners...</td>
      <td>REAL</td>
    </tr>
  </tbody>
</table>
</div>
<p>We find the shape of the dataframe to find how many articles are in this dataset (6335).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">news_df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(6335, 4)
</pre></div>
</div>
<p>We separate this data into three new datasets. titles_X includes all of the titles of our news articles in the same order as the original dataset. texts_X includes all of the texts of our news articles in the same order. news_y takes in the actual label given to the dataset for training purposes (Fake or Real).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">titles_X</span> <span class="o">=</span> <span class="n">news_df</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span>
<span class="n">texts_X</span> <span class="o">=</span> <span class="n">news_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="n">news_y</span> <span class="o">=</span> <span class="n">news_df</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>While scikit-learn will be able to deal with labels directly like REAL and FAKE, we convert these labels to a binary 0 and 1 to easily understand this conversion (instead of determining later what the algorithm had chosen for 0 and 1).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">news_y</span> <span class="o">=</span> <span class="n">news_y</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="s1">&#39;FAKE&#39;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;REAL&#39;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>/var/folders/xf/qc8rdk_57079zlfj9cc9fqvw0000gn/T/ipykernel_5440/2215345829.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option(&#39;future.no_silent_downcasting&#39;, True)`
  news_y = news_y.replace({&#39;FAKE&#39; : 0, &#39;REAL&#39; : 1})
</pre></div>
</div>
<p>Here we convert our list of titles and texts to a vector matrix. This vector matrix represents the patterns of words in these articles.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">count_vec</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">titles_X_vec</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">titles_X</span><span class="p">)</span>
<span class="n">texts_X_vec</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">texts_X</span><span class="p">)</span>
</pre></div>
</div>
<p>We can analyze the result of this through the shape attribute of our vector matrix. The first number (# of rows) tells us how many articles are present and is equal to the number of rows in the original dataframe. The second number (# of cols) tells us how many unique words are present among all article titles (or texts respectively). Each value in our matrix will represent a count of the occurences of each unique word in each article.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">titles_X_vec</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">texts_X_vec</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>((6335, 10071), (6335, 67659))
</pre></div>
</div>
<p>Before we split into testing and training data, we will convert our vectors to another format. Here we convert our vectors into TD-IDF, which keeps into account not only word frequency, but the frequency among our total documents. This will allow us to compare vocabulary representations and optimize to see which is better suited for our classifier.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tfidf</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">TfidfTransformer</span><span class="p">()</span>
<span class="n">titles_X_tfidf</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">titles_X_vec</span><span class="p">)</span>
<span class="n">texts_X_tfidf</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">texts_X_vec</span><span class="p">)</span>
</pre></div>
</div>
<p>We can notice that the tfidf matrix has the same shape as the vector matricies for both titles and texts. This is because the rows and cols are similar, but with an extra parameter in its subsequent calculation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">titles_X_tfidf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(6335, 10071)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">texts_X_tfidf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(6335, 67659)
</pre></div>
</div>
<p>Here we split our <strong>titles_X_vec</strong> representation and our <strong>titles_X_tfidf</strong> representation into training and testing data, alongside the usual y output. We do this all at once for our titles and our texts in order to compare more fairly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">titles_vec_train</span><span class="p">,</span> <span class="n">titles_vec_test</span><span class="p">,</span> <span class="n">texts_vec_train</span><span class="p">,</span> <span class="n">texts_vec_test</span><span class="p">,</span> <span class="n">titles_tfidf_train</span><span class="p">,</span> <span class="n">titles_tfidf_test</span><span class="p">,</span> <span class="n">texts_tfidf_train</span><span class="p">,</span> <span class="n">texts_tfidf_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">titles_X_vec</span><span class="p">,</span> <span class="n">texts_X_vec</span><span class="p">,</span> <span class="n">titles_X_tfidf</span><span class="p">,</span> <span class="n">texts_X_tfidf</span><span class="p">,</span> <span class="n">news_y</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">17</span><span class="p">)</span>
</pre></div>
</div>
<p>We then instantiate our classification models, one each for the vec and tfidf representations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf_vec_titles</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="n">clf_vec_texts</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="n">clf_tfidf_titles</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">clf_tfidf_texts</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
</pre></div>
</div>
<p>This next step fits out classifier model with the titles vector training data, and subsequently scores the model. As usual, the fitting is done with X and y training data, while the scoring is done with X and y testing data.</p>
<section id="titles-vector-representation-score">
<h2>Titles Vector Representation Score<a class="headerlink" href="#titles-vector-representation-score" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf_vec_titles</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">titles_vec_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">titles_vec_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.8251262626262627
</pre></div>
</div>
</section>
<section id="texts-vector-representation-score">
<h2>Texts Vector Representation Score<a class="headerlink" href="#texts-vector-representation-score" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf_vec_texts</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">texts_vec_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">texts_vec_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.8813131313131313
</pre></div>
</div>
</section>
<section id="titles-tf-idf-representation-score">
<h2>Titles TF-IDF Representation Score<a class="headerlink" href="#titles-tf-idf-representation-score" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf_tfidf_titles</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">titles_tfidf_train</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">titles_tfidf_test</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.6849747474747475
</pre></div>
</div>
</section>
<section id="texts-tf-idf-representation-score">
<h2>Texts TF-IDF Representation Score<a class="headerlink" href="#texts-tf-idf-representation-score" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf_tfidf_texts</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">texts_tfidf_train</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">texts_tfidf_test</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.7853535353535354
</pre></div>
</div>
<p>After fitting and scoring our four scenarios (texts and titles, both with a vector and tf-idf representation, we have a preliminary observation that the Texts of an article can be the most predictive of its realness or fakeness. However, this is only characterizable in a vector representation of its vocabulary, and NOT with the tf-idf representation, which is surprising.</p>
<p>While these are interesting results, it would be more valuable if we performed a cross-validation analysis. This is because these scores are random based on the state in which we split our data into training and testing dataframes. A cross validation will give us a chance to find an average of multiple splits. I have added the import at the beginning of the mark down so we can use a simple cross validation.</p>
<p>We have to use our vector and tfidf dataframes, BEFORE we split into training and testing data, so we pass in the complete titles_X_vec, et cetera. Because the dataset is quite large, we can use a larger amount of folds that give us an interesting representation.</p>
</section>
<section id="titles-vector-cross-validation-score">
<h2>Titles Vector Cross-Validation Score<a class="headerlink" href="#titles-vector-cross-validation-score" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf_vec_titles</span><span class="p">,</span> <span class="n">titles_X_vec</span><span class="p">,</span> <span class="n">news_y</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.8209906259811323
</pre></div>
</div>
</section>
<section id="texts-vector-cross-validation-score">
<h2>Texts Vector Cross-Validation Score<a class="headerlink" href="#texts-vector-cross-validation-score" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf_vec_texts</span><span class="p">,</span> <span class="n">texts_X_vec</span><span class="p">,</span> <span class="n">news_y</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.8865023597011877
</pre></div>
</div>
</section>
<section id="titles-tf-idf-cross-validation-score">
<h2>Titles TF-IDF Cross-Validation Score<a class="headerlink" href="#titles-tf-idf-cross-validation-score" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf_tfidf_titles</span><span class="p">,</span> <span class="n">titles_X_tfidf</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">news_y</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.6915541634896667
</pre></div>
</div>
</section>
<section id="texts-tf-idf-cross-validation-score">
<h2>Texts TF-IDF Cross-Validation Score<a class="headerlink" href="#texts-tf-idf-cross-validation-score" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf_tfidf_texts</span><span class="p">,</span> <span class="n">texts_X_tfidf</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">news_y</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.8050453252002132
</pre></div>
</div>
<p>These results are very similar to our results for one specific training and testing set, but now we have proved that it remains consistent for an average across different splits. We can say with much better certainty that while all scenarios are fairly good at model this prediction, the prediction is more effective using article texts with a vector representation.</p>
<p>Based on our above analysis, we have determined that texts are more predictive of the legitimacy of an article, however only for a certain representation. We have tested the titles and the texts of an article separately with two different representations, a count-vector matrix and a term frequency-inverse document frequency (tfidf) vector matrix. Among these four scenarios total, we have found that the highest predictive score (after a 10-fold cross validation) was the article texts as a count-vector matrix. This had an accuracy score of about 0.88 on average. Given that this analysis was done over a 10-fold cross validation, I would suggest that there is enough evidence to support this claim, as this was a much higher score that is repeatable over a computer average. While the text count-vectorization representation is the most predictive, we see that under a tf-idf comparison, it is much lower. On a 10-fold average, we see that the tf-idf performs at about 80% success for text, significantly lower that the count-vector’s 88%. Surprisingly at first look, the title performed at about 82% in the vector representation, but much lower at about 69% under the TF-IDF representation and the Gaussian Naive Bayes Model. I would expect that this is because of the lack of a vocabulary in titles, meaning simply less words to aid in prediction. This would also explain the lesser score for the count-vector representation.</p>
<p>We can look at both the title the text analysis to find which representation is better. This ends up being the count-vector, with the optimal score of 0.88 for the vector when compared to 0.80 for the tf-idf representation. For the title analysis, this is even greater, where we see a roughly 0.82 score for the vector over 0.69 for the tf-idf. And with that comparison, we can further say that not only is the text of an article more predictive of an article’s legitimacy, but the count vector representation is better for model training than the tf-idf. This could also further conclude, that not only is the count vector representation better than the tf-idf representation, but the Multinomial Naive Bayes model may be more effective than the Gaussian Naive Bayes model for this analysis. While Gaussian Naive Bayes is used to better utilize the TF-IDF representation, it is still not as effective as the use of the Multinomial Naive Bayes model for the count-vector representation.</p>
</section>
<section id="model-optimization">
<h2>Model Optimization<a class="headerlink" href="#model-optimization" title="Permalink to this heading">#</a></h2>
<p>We will then optimize our best model, to determine if we can achive an optimal cross validation score. The model we will use is our article texts in a count-vector representation with a Multinomial Naive Bayes model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf_vec_texts</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
</pre></div>
</div>
<p>We choose to increment alpha by factors of 10 in order to get a general range of where this parameter should lie.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_dt</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.00001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span><span class="mi">1000</span><span class="p">]}</span>
</pre></div>
</div>
<p>Using our vec_texts classifer (MultinomialNB), we optimize the model with our parameters and get a resultant GridSearch with a cross-validation of 10.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf_vec_texts_opt</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">clf_vec_texts</span><span class="p">,</span> <span class="n">param_dt</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf_vec_texts_opt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">texts_X_vec</span><span class="p">,</span> <span class="n">news_y</span><span class="p">)</span>
</pre></div>
</div>
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=10, estimator=MultinomialNB(),
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>         param_grid={&amp;#x27;alpha&amp;#x27;: [1e-05, 0.0001, 0.001, 0.1, 1, 10, 100,
                               1000]})&lt;/pre&gt;&lt;b&gt;In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. &lt;br /&gt;On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;sk-container&quot; hidden&gt;&lt;div class=&quot;sk-item sk-dashed-wrapped&quot;&gt;&lt;div class=&quot;sk-label-container&quot;&gt;&lt;div class=&quot;sk-label fitted sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-1&quot; type=&quot;checkbox&quot; &gt;&lt;label for=&quot;sk-estimator-id-1&quot; class=&quot;sk-toggleable__label fitted sk-toggleable__label-arrow fitted&quot;&gt;&amp;nbsp;&amp;nbsp;GridSearchCV&lt;a class=&quot;sk-estimator-doc-link fitted&quot; rel=&quot;noreferrer&quot; target=&quot;_blank&quot; href=&quot;https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html&quot;&gt;?&lt;span&gt;Documentation for GridSearchCV&lt;/span&gt;&lt;/a&gt;&lt;span class=&quot;sk-estimator-doc-link fitted&quot;&gt;i&lt;span&gt;Fitted&lt;/span&gt;&lt;/span&gt;&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content fitted&quot;&gt;&lt;pre&gt;GridSearchCV(cv=10, estimator=MultinomialNB(),
         param_grid={&amp;#x27;alpha&amp;#x27;: [1e-05, 0.0001, 0.001, 0.1, 1, 10, 100,
                               1000]})&lt;/pre&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;sk-parallel&quot;&gt;&lt;div class=&quot;sk-parallel-item&quot;&gt;&lt;div class=&quot;sk-item&quot;&gt;&lt;div class=&quot;sk-label-container&quot;&gt;&lt;div class=&quot;sk-label fitted sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-2&quot; type=&quot;checkbox&quot; &gt;&lt;label for=&quot;sk-estimator-id-2&quot; class=&quot;sk-toggleable__label fitted sk-toggleable__label-arrow fitted&quot;&gt;best_estimator_: MultinomialNB&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content fitted&quot;&gt;&lt;pre&gt;MultinomialNB(alpha=0.0001)&lt;/pre&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;sk-serial&quot;&gt;&lt;div class=&quot;sk-item&quot;&gt;&lt;div class=&quot;sk-estimator fitted sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-3&quot; type=&quot;checkbox&quot; &gt;&lt;label for=&quot;sk-estimator-id-3&quot; class=&quot;sk-toggleable__label fitted sk-toggleable__label-arrow fitted&quot;&gt;&amp;nbsp;MultinomialNB&lt;a class=&quot;sk-estimator-doc-link fitted&quot; rel=&quot;noreferrer&quot; target=&quot;_blank&quot; href=&quot;https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html&quot;&gt;?&lt;span&gt;Documentation for MultinomialNB&lt;/span&gt;&lt;/a&gt;&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content fitted&quot;&gt;&lt;pre&gt;MultinomialNB(alpha=0.0001)&lt;/pre&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</pre></div>
</div>
<p>We find that the most optimized parameters of alpha is 0.0001</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf_vec_texts_opt</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;alpha&#39;: 0.0001}
</pre></div>
</div>
<p>We then score our optimize model to see how successful it is in determining the legitimacy of our articles. We find that this optimized model gives a score of 0.974, which is very good! Because of this cross-validation score, I would absolutely recommend the deployment of this model and would be confident saying that the count vectorization representation of article texts under a Multnomial Naive Bayes classification model is the most effective in determining an article legitimacy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf_vec_texts_opt</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">texts_vec_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.9741161616161617
</pre></div>
</div>
</section>
<section id="data-visualization">
<h2>Data Visualization<a class="headerlink" href="#data-visualization" title="Permalink to this heading">#</a></h2>
<p>For this next step, we will then try to visualize our data using a heatmap. We first grab the indices respectively where news_y correlates to fake news, and then real news. We concatenate so we can have all fake news articles listed first, and then all real news articles.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fake_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">news_y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">real_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">news_y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">subset_rows</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">fake_indices</span><span class="p">,</span> <span class="n">real_indices</span><span class="p">])</span>
</pre></div>
</div>
<p>Here we grab the lengths of our fake_indices and real_indices to make sure that our heatmap will more or less contain even quadrants.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">fake_indices</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">real_indices</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(3164, 3171)
</pre></div>
</div>
<p>The number of fake articles to real articles is similar, which is good for a heatmap representation, giving us four distinct quadrants.</p>
<p>Doing this separation and subsequent concatenation allows us to model our heatmap with distinct quadrants. In the top-left quadrant we compare fake news articles with other fake news articles. The bottom-right quadrant compares real news articles with other real news articles. The other two quadrants compares real and fake news articles with each other. We perform this on both the titles and texts of the tfidf representation as it is not an effective visualization for count vector representations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">titles_X_tfidf</span><span class="p">[</span><span class="n">subset_rows</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;Axes: &gt;
</pre></div>
</div>
<p><img alt="png" src="assignment12_files/assignment12_66_1.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">texts_X_tfidf</span><span class="p">[</span><span class="n">subset_rows</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;Axes: &gt;
</pre></div>
</div>
<p><img alt="png" src="assignment12_files/assignment12_67_1.png" /></p>
<p>When looking at the heat maps, we see that titles are not a good indicator of different in their euclidean distances as representations are fairly homogenous. However, when looking at texts_tfidfs, we see that there is a general darker color in the bottom right quadrant. This means that real news articles are more similar in their vocabulary when compared to fake news, which tends to use much more varied terms.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">selected_categories</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Fake&#39;</span><span class="p">,</span> <span class="s1">&#39;Real&#39;</span><span class="p">]</span>
<span class="n">texts_y_pred</span> <span class="o">=</span> <span class="n">clf_tfidf_texts</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">texts_tfidf_test</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
<span class="n">cols</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">MultiIndex</span><span class="o">.</span><span class="n">from_arrays</span><span class="p">([[</span><span class="s1">&#39;actual class&#39;</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">selected_categories</span><span class="p">),</span>
                                  <span class="n">selected_categories</span><span class="p">])</span>
<span class="n">rows</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">MultiIndex</span><span class="o">.</span><span class="n">from_arrays</span><span class="p">([[</span><span class="s1">&#39;predicted class&#39;</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">selected_categories</span><span class="p">),</span>
                                  <span class="n">selected_categories</span><span class="p">])</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">texts_y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span><span class="n">columns</span> <span class="o">=</span> <span class="n">cols</span><span class="p">,</span>
             <span class="n">index</span> <span class="o">=</span> <span class="n">rows</span><span class="p">)</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th colspan="2" halign="left">actual class</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th>Fake</th>
      <th>Real</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2" valign="top">predicted class</th>
      <th>Fake</th>
      <td>585</td>
      <td>149</td>
    </tr>
    <tr>
      <th>Real</th>
      <td>191</td>
      <td>659</td>
    </tr>
  </tbody>
</table>
</div>
<p>This falls in line with our general analysis from the heat map. Since fake news articles have more varied vocabularies when compared to real news (albeit slightly), we would expect that the classifer would predict fakes news wrongly more than real news articles. As expected, this is demonstrated by our confusion matrix. We see that fake news was predicted as being real about 190 times. Real news however was predicted as being fake just under 150 times. While these values are fairly similar, the difference is still modeled and agrees with the heat map.</p>
<p>The heatmap given above contains four quadrants in which each is a comparison of either real to real, fake to fake, or across distinctions. We see that the vocabularies are quite varied across titles, as expected, regardless of its legitimacy. However, among article texts, we can observe that fake articles have a more varied vocabulary than real articles. This varied vocabulary in fake articles contributes to misclassification as determined by the confusion matrix in which more fake articles were classified as real than vice versa. This means that texts of real articles are more similar to each other, giving more room for fake articles to ‘mimic’ their vocabulary. Titles, however, seem to be different for more fake and real articles, making it hard to use that as a classifier (hence the lower success rate in cross validation).</p>
<p>We then decide to calculate the average euclidean distance for each quadrant and display the data in a pandas DataFrame. We do this to determine if there is noticeable significance in the variance of vocabularies among the types of articles. We have assumed that there is significant difference, but this will allow us to determine what exactly that average distance is.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">distances</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">titles_X_tfidf</span><span class="p">[</span><span class="n">subset_rows</span><span class="p">])</span>

<span class="n">fake_v_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">distances</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">fake_indices</span><span class="p">),</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">fake_indices</span><span class="p">)])</span>
<span class="n">real_v_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">distances</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">fake_indices</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">fake_indices</span><span class="p">)])</span>
<span class="n">fake_v_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">distances</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">fake_indices</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">fake_indices</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:])</span>
<span class="n">real_v_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">distances</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">fake_indices</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span> <span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">fake_indices</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:])</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span> <span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="n">fake_v_fake</span><span class="p">,</span> <span class="n">real_v_fake</span><span class="p">],</span> <span class="p">[</span><span class="n">fake_v_real</span><span class="p">,</span> <span class="n">real_v_real</span><span class="p">]],</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Fake&#39;</span><span class="p">,</span> <span class="s1">&#39;Real&#39;</span><span class="p">],</span> <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Fake&#39;</span><span class="p">,</span> <span class="s1">&#39;Real&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Fake</th>
      <th>Real</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Fake</th>
      <td>1.405605</td>
      <td>1.406904</td>
    </tr>
    <tr>
      <th>Real</th>
      <td>1.406904</td>
      <td>1.404685</td>
    </tr>
  </tbody>
</table>
</div>
<p>It turns out there actually is not much of a difference between the distances of fake/fake or real/real article relationships. The average difference between tfidf vocaularies of these articles are similar regardless of their legitimacy identification. This would explain why the TF-IDF did not perform as effectively as the vector representation, as their adjusted vocabulary representations are not very different across articles.</p>
<p>Finally, to show the similarity between the distances of these quadrants, we will plot a simple matplot. Here we have the type of euclidean distance comparison on the x-axis, and the average distance between article of the y-axis. We start by organizing the dataframe in a different format for better accessibility in seaborn.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mean_distances</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span> <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">fake_v_fake</span><span class="p">,</span> <span class="n">real_v_fake</span><span class="p">,</span> <span class="n">fake_v_real</span><span class="p">,</span> <span class="n">real_v_real</span><span class="p">],</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Mean Euclidean Distance&#39;</span><span class="p">],</span> <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Fake v Fake&#39;</span><span class="p">,</span> <span class="s1">&#39;Real v Fake&#39;</span><span class="p">,</span> <span class="s1">&#39;Fake v Real&#39;</span><span class="p">,</span> <span class="s1">&#39;Real v Real&#39;</span><span class="p">])</span>
<span class="n">mean_distances</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Mean Euclidean Distance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Fake v Fake</th>
      <td>1.405605</td>
    </tr>
    <tr>
      <th>Real v Fake</th>
      <td>1.406904</td>
    </tr>
    <tr>
      <th>Fake v Real</th>
      <td>1.406904</td>
    </tr>
    <tr>
      <th>Real v Real</th>
      <td>1.404685</td>
    </tr>
  </tbody>
</table>
</div>
<p>We then create the seaborn plot.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">md_plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">mean_distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">mean_distances</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;Mean Euclidean Distance&#39;</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="s2">&quot;Mean Euclidean Distance&quot;</span><span class="p">,</span> <span class="n">legend</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">md_plot</span><span class="o">.</span><span class="n">containers</span><span class="p">:</span>
    <span class="n">md_plot</span><span class="o">.</span><span class="n">bar_label</span><span class="p">(</span><span class="n">i</span><span class="p">,)</span>
</pre></div>
</div>
<p><img alt="png" src="assignment12_files/assignment12_78_0.png" /></p>
<p>As we can see, the mean euclidean distance between the types of articles are almost equivalent, suggesting a similarity in TF-IDF representations which may be contributing to their lesser scores.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="a10_comparison.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">&lt;no title&gt;</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#titles-vector-representation-score">Titles Vector Representation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#texts-vector-representation-score">Texts Vector Representation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#titles-tf-idf-representation-score">Titles TF-IDF Representation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#texts-tf-idf-representation-score">Texts TF-IDF Representation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#titles-vector-cross-validation-score">Titles Vector Cross-Validation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#texts-vector-cross-validation-score">Texts Vector Cross-Validation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#titles-tf-idf-cross-validation-score">Titles TF-IDF Cross-Validation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#texts-tf-idf-cross-validation-score">Texts TF-IDF Cross-Validation Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-optimization">Model Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-visualization">Data Visualization</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Logan McCue
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>